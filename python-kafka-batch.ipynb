{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b433625-0622-493d-8e47-32a3cf1e3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample code to send messages to Kafka\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "bootstrap_servers = 'kafka:29092'\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "topic = 'test_t'\n",
    "messages = [\n",
    "    '<?xml version=\"1.0><filmes><filme id=\"4\"><titulo>O XML veste prada</titulo><resumo>O filme mostra a elegância da XML na representação de dados estruturados e semi estruturados.</resumo><genero>Aventura</genero><genero>Documentário</genero><elenco><ator>Mark UPlanguage</ator><ator>Mary well-Formed</ator><ator>Sedna D. Atabase</ator></elenco></filme></filmes>',\n",
    "    '<?xml version=\"1.0><filmes><filme id=\"5\"><titulo>O XML veste prada</titulo><resumo>O filme mostra a elegância da XML na representação de dados estruturados e semi estruturados.</resumo><genero>Aventura</genero><genero>Documentário</genero><elenco><ator>Mark UPlanguage</ator><ator>Mary well-Formed</ator><ator>Sedna D. Atabase</ator></elenco></filme></filmes>',\n",
    "    '<?xml version=\"1.0><filmes><filme id=\"6\"><titulo>O XML veste prada</titulo><resumo>O filme mostra a elegância da XML na representação de dados estruturados e semi estruturados.</resumo><genero>Aventura</genero><genero>Documentário</genero><elenco><ator>Mark UPlanguage</ator><ator>Mary well-Formed</ator><ator>Sedna D. Atabase</ator></elenco></filme></filmes>'\n",
    "]\n",
    "count = 1\n",
    "while count > 0:\n",
    "    for message in messages:\n",
    "        producer.send(topic, message.encode('utf-8'))\n",
    "    count = count - 1\n",
    "\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e50992d-2402-468d-9c30-b869c825cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using KafkaConsumer and kafka-python\n",
    "## Access docker shell and execute pip install kafka-python\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e901488-d140-4d5b-834e-acb277721353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fuction to receive a dict and parse message\n",
    "def record_to_dict(record):\n",
    "    value = record.value.decode('utf-8')\n",
    "    return {\n",
    "        \"topic\": str(record.topic),\n",
    "        \"partition\": int(record.partition),\n",
    "        \"offset\": int(record.offset),\n",
    "        \"timestamp\": int(record.timestamp),\n",
    "        \"timestamp_type\": int(record.timestamp_type),\n",
    "        \"key\": str(record.key),\n",
    "        \"value\": str(value),\n",
    "        \"headers\": str(record.headers),\n",
    "        \"checksum\": str(record.checksum),\n",
    "        \"serialized_key_size\": int(record.serialized_key_size),\n",
    "        \"serialized_value_size\": int(record.serialized_value_size),\n",
    "        \"serialized_header_size\": int(record.serialized_header_size)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecacca15-ca62-4f49-9400-5a3da2074836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cleanup dataframe and keep only 'offset', 'timestamp', 'key', 'value'\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def cleanup_dataframe(data):\n",
    "    spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "    spark_df = spark.createDataFrame(data)\n",
    "    selected_columns = [\"offset\", \"timestamp\", \"key\", \"value\"]\n",
    "    spark_df = spark_df.select(selected_columns)\n",
    "    parquet_path = \"/tmp/parquet_file.parquet\"\n",
    "    spark_df.write.mode('append').parquet(parquet_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a40007af-f05b-455c-baf6-76455aa383b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consumer Function to retrieve messages from topic: \n",
    "## Function receive topic as parameter\n",
    "\n",
    "def consumer(topic_name):\n",
    "    try:\n",
    "        consumer = KafkaConsumer(bootstrap_servers='kafka:29092',\n",
    "                                 group_id='my-ssdgroupss',\n",
    "                                 auto_offset_reset = \"latest\",\n",
    "                                 enable_auto_commit = \"false\",\n",
    "                                 max_poll_records = 100\n",
    "                                )\n",
    "\n",
    "        consumer.subscribe([f'{topic_name}'])\n",
    "        total_messages = 0\n",
    "\n",
    "        # keep track of when we last received a message\n",
    "        last_message_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            messages = consumer.poll(timeout_ms=10000).values()\n",
    "            if messages:\n",
    "                flat_messages = list(itertools.chain.from_iterable(messages))\n",
    "                data = list(map(record_to_dict, flat_messages))\n",
    "                df = pd.DataFrame(data)\n",
    "                df = cleanup_dataframe(df)\n",
    "                total_messages += len(flat_messages)\n",
    "                # reset the last message time\n",
    "                last_message_time = time.time()\n",
    "            \n",
    "            # If we have not received a message for 10 seconds, stop consuming\n",
    "            if time.time() - last_message_time > 10:\n",
    "                print(f'Total messages received:{total_messages}')\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "\n",
    "    finally:\n",
    "        # After finished consumer will be closed, make sure that all messages were commited.\n",
    "        consumer.commit()\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "999f5211-8ea6-45c8-bd02-420d220f482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages received:15\n"
     ]
    }
   ],
   "source": [
    "## Call function as batch to read topic and need topic as parameter\n",
    "consumer('test_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bce05d3d-ea13-43f5-b961-5646a67408d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to read parquet file\n",
    "def read_parquet(path):\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Read Parquet\") \\\n",
    "        .getOrCreate()\n",
    "    df = spark.read.parquet(path)\n",
    "    df.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00fe1ca4-2e97-4693-8326-f1ee2bbd86b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----+--------------------+\n",
      "| offset|    timestamp| key|               value|\n",
      "+-------+-------------+----+--------------------+\n",
      "|3038918|1686786509679|None|<?xml version=\"1....|\n",
      "|3038907|1686786509678|None|<?xml version=\"1....|\n",
      "|3038906|1686786509678|None|<?xml version=\"1....|\n",
      "|3038932|1686786540870|None|<?xml version=\"1....|\n",
      "|3038904|1686786471982|None|<?xml version=\"1....|\n",
      "|3038933|1686786540870|None|<?xml version=\"1....|\n",
      "|3038913|1686786509679|None|<?xml version=\"1....|\n",
      "|3038922|1686786540870|None|<?xml version=\"1....|\n",
      "|3038927|1686786540870|None|<?xml version=\"1....|\n",
      "|3038908|1686786509678|None|<?xml version=\"1....|\n",
      "|3038926|1686786540870|None|<?xml version=\"1....|\n",
      "|3038905|1686786471982|None|<?xml version=\"1....|\n",
      "|3038928|1686786540870|None|<?xml version=\"1....|\n",
      "|3038917|1686786509679|None|<?xml version=\"1....|\n",
      "|3038912|1686786509679|None|<?xml version=\"1....|\n",
      "|3038911|1686786509678|None|<?xml version=\"1....|\n",
      "|3038916|1686786509679|None|<?xml version=\"1....|\n",
      "|3038931|1686786540870|None|<?xml version=\"1....|\n",
      "|3038921|1686786540870|None|<?xml version=\"1....|\n",
      "|3038903|1686786471982|None|<?xml version=\"1....|\n",
      "+-------+-------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample function to read parquet and print as dataframe\n",
    "parquet_path = \"/tmp/parquet_file.parquet\"\n",
    "df = read_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427534d8-9471-4146-ab58-8bf4a6991aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
